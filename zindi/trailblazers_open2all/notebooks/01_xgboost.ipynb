{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1us8_EEbrVRd-VEC-HmynjqAgDTA3KiiE",
      "authorship_tag": "ABX9TyO53dVSmwzlSeiM/imgZUq4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stiwari-ds/data-science-competitions/blob/main/zindi/trailblazers_open2all/notebooks/01_xgboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "fMe_ffy2a_ac"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wyde65PCala3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install --upgrade xgboost\n",
        "!pip install --upgrade optuna"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import warnings\n",
        "import subprocess\n",
        "\n",
        "gc.enable()\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.precision', 4)\n",
        "np.set_printoptions(precision=4)\n",
        "\n",
        "import xgboost\n",
        "import optuna\n",
        "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
        "\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.pruners import HyperbandPruner\n",
        "from optuna.integration import XGBoostPruningCallback\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "from sklearn.model_selection import KFold, GroupKFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "SEED = 23\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
        "np.random.seed(SEED)"
      ],
      "metadata": {
        "id": "eKfBaXpBbBUQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove cell to run future versions\n",
        "assert optuna.__version__ == '3.0.2', f'Change in Optuna version. Original notebook version: 3.0.2'\n",
        "assert xgboost.__version__ == '1.6.2', f'Change in XGBoost version. Original notebook version: 1.6.2'"
      ],
      "metadata": {
        "id": "QU6c1orQbm3A"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check GPU availability\n",
        "try:\n",
        "    subprocess.check_output('nvidia-smi')\n",
        "    HAVE_GPU = True\n",
        "except Exception:\n",
        "    HAVE_GPU = False\n",
        "\n",
        "print(f'GPU available: {HAVE_GPU}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBzLRr5pbr7Z",
        "outputId": "03a69911-6863-4cbf-92b9-e0ffa8100e0f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_o0kubsb5bP",
        "outputId": "6f2829b1-69ef-4ad1-e46a-da0551931596"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = '/content/drive/MyDrive/data_science_competitions/zindi/trailblazers_open2all/data'\n",
        "train = pd.read_csv(f'{DATA_PATH}/raw/train.csv')\n",
        "test = pd.read_csv(f'{DATA_PATH}/raw/test.csv')\n",
        "sample_sub = pd.read_csv(f'{DATA_PATH}/raw/sample_sub.csv')"
      ],
      "metadata": {
        "id": "Ts7dyuJcb59B"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NOTEBOOK = '01'\n",
        "SUBMISSION_PATH = f'/content/drive/MyDrive/data_science_competitions/zindi/trailblazers_open2all/submissions/nb_{NOTEBOOK}'\n",
        "if not os.path.isdir(SUBMISSION_PATH):\n",
        "    os.makedirs(SUBMISSION_PATH)"
      ],
      "metadata": {
        "id": "Fgq12dmvcBN_"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "3rK34PzocQb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET = train['target']\n",
        "TEST_INDEX = test['Place_ID X Date'] #for submission files\n",
        "GROUPS = np.array(train['Place_ID']) #for GroupKFold cross-validation"
      ],
      "metadata": {
        "id": "m51lh19QHiPF"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df: pd.DataFrame, is_train: bool = False) -> pd.DataFrame:\n",
        "    #Convert date column to datetime type\n",
        "    df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "    #Create date-based features\n",
        "    df['month'] = df['Date'].dt.month\n",
        "    df['day'] = df['Date'].dt.day\n",
        "    df['day_of_week'] = df['Date'].dt.day_of_week\n",
        "\n",
        "    #dropping non-feature columns\n",
        "    df = df.drop(labels=['Place_ID X Date', 'Place_ID', 'Date'], axis=1)\n",
        "    if is_train:\n",
        "        df = df.drop(\n",
        "            labels=['target', 'target_min', 'target_max', 'target_variance', 'target_count'], \n",
        "            axis=1\n",
        "        )\n",
        "    \n",
        "    #reduce memory usage\n",
        "    def reduce_mem(df: pd.DataFrame) -> pd.DataFrame:\n",
        "        for col in df.columns:\n",
        "            col_type = df[col].dtypes\n",
        "            if col_type in ['int64', 'float64']:\n",
        "                c_min = df[col].min()\n",
        "                c_max = df[col].max()\n",
        "                if str(col_type).startswith('int'):\n",
        "                    if c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "                        df[col] = df[col].astype(np.int16)\n",
        "                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "                        df[col] = df[col].astype(np.int32)\n",
        "                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
        "                        df[col] = df[col].astype(np.int64)\n",
        "                else:\n",
        "                    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
        "                        df[col] = df[col].astype(np.float16)\n",
        "                    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
        "                        df[col] = df[col].astype(np.float32)\n",
        "                    else:\n",
        "                        df[col] = df[col].astype(np.float64)\n",
        "        return df\n",
        "    \n",
        "    return reduce_mem(df)"
      ],
      "metadata": {
        "id": "2X_UZP6LcS3L"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = preprocess(train, is_train=True)\n",
        "test = preprocess(test)\n",
        "\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c_VjXFqVDzX",
        "outputId": "1e5192d9-75bc-4997-b36c-ce6ae10bc22e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "115"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline"
      ],
      "metadata": {
        "id": "l-ZLgtRMrJDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "scores_rmse = []\n",
        "cv = GroupKFold(n_splits=5)\n",
        "X, y = train, TARGET\n",
        "for fold, (train_idx, val_idx) in enumerate(cv.split(X, y, GROUPS)):\n",
        "    X_train, y_train = X.loc[train_idx], y.iloc[train_idx]\n",
        "    X_val, y_val = X.loc[val_idx], y.iloc[val_idx]\n",
        "\n",
        "    model = XGBRegressor(\n",
        "        objective='reg:squarederror',\n",
        "        tree_method='gpu_hist' if HAVE_GPU else 'hist',\n",
        "        eval_metric='rmse',\n",
        "        early_stopping_rounds=50, \n",
        "        seed=SEED\n",
        "    ) \n",
        "    model.fit(\n",
        "        X_train, y_train,\n",
        "        eval_set=[(X_val, y_val)],\n",
        "        verbose=0\n",
        "    )\n",
        "    val_preds = model.predict(X_val)\n",
        "    score = mean_squared_error(y_val, val_preds, squared=False)\n",
        "    scores_rmse.append(score)\n",
        "    print(f'Fold #{fold}: ({model.best_iteration} rounds) RMSE = {score:.5f}')\n",
        "    _ = gc.collect()\n",
        "\n",
        "print(f'\\nAvg RMSE = {np.mean(scores_rmse):.5f} +/- {np.std(scores_rmse):.5f}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccFthHeorwUe",
        "outputId": "60c92638-fa00-4068-bd3e-161b79af8575"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold #0: (99 rounds) RMSE = 35.43706\n",
            "Fold #1: (98 rounds) RMSE = 32.89088\n",
            "Fold #2: (69 rounds) RMSE = 29.80391\n",
            "Fold #3: (98 rounds) RMSE = 38.16264\n",
            "Fold #4: (94 rounds) RMSE = 31.06374\n",
            "\n",
            "Avg RMSE = 33.47165 +/- 3.01516\n",
            "\n",
            "CPU times: user 5.43 s, sys: 325 ms, total: 5.76 s\n",
            "Wall time: 5.26 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVZEqwhQ2oi2"
      },
      "source": [
        "# Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "i3VeEFRe2oBe"
      },
      "outputs": [],
      "source": [
        "def objective(trial, data, base_params):\n",
        "\n",
        "    scores = []\n",
        "    X, y = data\n",
        "\n",
        "    param_grid = {\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, step=0.01),\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 2, 20),\n",
        "        'gamma': trial.suggest_float('gamma', 0, 20, step=0.1), #complexity-control\n",
        "        'alpha': trial.suggest_float('alpha', 0, 5, step=0.05), #L1-reg\n",
        "        'lambda': trial.suggest_float('lambda', 1e-3, 1e5, log=True), #L2-reg\n",
        "        # 'subsample': trial.suggest_float('subsample', 0.5, 1.0, step=0.05),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0, step=0.05),\n",
        "        'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0, step=0.05),\n",
        "        'colsample_bynode': trial.suggest_float('colsample_bynode', 0.5, 1.0, step=0.05)\n",
        "    }\n",
        "\n",
        "    cv = GroupKFold(n_splits=5)\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y, GROUPS)):\n",
        "        X_train, y_train = X.loc[train_idx], y.iloc[train_idx]\n",
        "        X_val, y_val = X.loc[val_idx], y.iloc[val_idx]\n",
        "        model = XGBRegressor(\n",
        "            **base_params, \n",
        "            **param_grid,\n",
        "            callbacks=[XGBoostPruningCallback(\n",
        "                trial=trial, \n",
        "                observation_key='validation_0-rmse')]\n",
        "        )\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            verbose=0\n",
        "        )\n",
        "        preds = model.predict(X_val)\n",
        "        scores.append(mean_squared_error(y_val, preds, squared=False))\n",
        "    \n",
        "    return np.mean(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "7JCjxMSuT3Ep"
      },
      "outputs": [],
      "source": [
        "def tune_params(data, base_params, n_trials=10, direction='maximize'):\n",
        "    study = optuna.create_study(\n",
        "        sampler=TPESampler(seed=SEED),\n",
        "        pruner=HyperbandPruner(),\n",
        "        direction=direction\n",
        "    )\n",
        "    \n",
        "    study.optimize(\n",
        "        func=lambda trial: objective(trial, data, base_params),\n",
        "        n_trials=n_trials,\n",
        "        gc_after_trial=True\n",
        "    )\n",
        "    \n",
        "    return study"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-89LWIEHeTr"
      },
      "source": [
        "# Cross-validation and experiment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "OsTtaJBBYryk"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(data, model_params, verbose=True):\n",
        "    oof_preds = {}  #out-of-fold predictions on train set\n",
        "    test_preds = {} #predictions on test set for each fold\n",
        "    scores_rmse = [] #RMSE scores on validation set\n",
        "\n",
        "    X, X_test, y = data\n",
        "\n",
        "    cv = GroupKFold(n_splits=5)\n",
        "    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y, GROUPS)):\n",
        "        X_train, y_train = X.loc[train_idx], y.iloc[train_idx]\n",
        "        X_val, y_val = X.loc[val_idx], y.iloc[val_idx]\n",
        "        \n",
        "        model = XGBRegressor(**model_params)\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=[(X_val, y_val)],\n",
        "            verbose=0\n",
        "        )\n",
        "        val_preds = model.predict(X_val)\n",
        "        oof_preds.update(dict(zip(val_idx, val_preds)))\n",
        "        test_preds[f'fold{fold}'] = model.predict(X_test)\n",
        "\n",
        "        score = mean_squared_error(y_val, val_preds, squared=False)\n",
        "        scores_rmse.append(score)\n",
        "        if verbose:\n",
        "            print(f'Fold #{fold}: ({model.best_iteration} rounds) RMSE = {score:.5f}')\n",
        "        \n",
        "        _ = gc.collect()\n",
        "\n",
        "    print(f'\\nAvg RMSE = {np.mean(scores_rmse):.5f} +/- {np.std(scores_rmse):.5f}')\n",
        "    \n",
        "    oof_preds = pd.Series(oof_preds).sort_index()\n",
        "    print(f'OOF RMSE = {mean_squared_error(y, oof_preds, squared=False):.5f}')\n",
        "    \n",
        "    test_preds = pd.DataFrame.from_dict(test_preds)\n",
        "    return oof_preds, test_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "I0TOKMzxhRt-"
      },
      "outputs": [],
      "source": [
        "def run_experiment(data, n_trials=5):\n",
        "        \n",
        "    X, X_test, y = data\n",
        "    \n",
        "    base_params = {\n",
        "        'objective': 'reg:squarederror',\n",
        "        'n_estimators': 10000,\n",
        "        'booster': 'gbtree',\n",
        "        'eval_metric': 'rmse',\n",
        "        'early_stopping_rounds': 100,\n",
        "        'tree_method': 'gpu_hist' if HAVE_GPU else 'hist',\n",
        "        'predictor': 'gpu_predictor' if HAVE_GPU else 'cpu_predictor',\n",
        "        'verbosity': 1,\n",
        "        'seed': SEED\n",
        "    }\n",
        "    \n",
        "    print(f'---------------Hyperparameter tuning---------------')\n",
        "    study = tune_params(\n",
        "        data=(X, y), \n",
        "        base_params=base_params,\n",
        "        n_trials=n_trials,\n",
        "        direction='minimize'\n",
        "    )\n",
        "    print(f'Best trial: {study.best_trial.number} -> Best value(RMSE): {study.best_value:.5f}')\n",
        "    print(f'Best hyperparameters:')\n",
        "    for k, v in study.best_params.items():\n",
        "        print(f'{k:20} - {v}')\n",
        "    \n",
        "    model_params = {**base_params, **study.best_params}\n",
        "    print(f'-----------------Cross-validation------------------')\n",
        "    oof_preds, test_preds = evaluate_model(\n",
        "        data=(X, X_test, y), \n",
        "        model_params=model_params\n",
        "    )\n",
        "    return oof_preds, test_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All features, No imputation"
      ],
      "metadata": {
        "id": "PFsk_N6F-F7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "oof_preds, test_preds = run_experiment(data=(train, test, TARGET), n_trials=250)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zj7Zsc5s2YFC",
        "outputId": "f5ba0c52-fab3-49a6-c48b-b8194a484e1d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2022-09-24 05:30:40,624]\u001b[0m A new study created in memory with name: no-name-9c65da42-d599-4d5d-920c-9387b277f7ce\u001b[0m\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------Hyperparameter tuning---------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-09-24 05:34:03,285]\u001b[0m Trial 0 finished with value: 31.20323621889596 and parameters: {'learning_rate': 0.16, 'max_depth': 15, 'min_child_weight': 16, 'gamma': 5.6000000000000005, 'alpha': 1.1, 'lambda': 308.87067834937415, 'colsample_bytree': 0.55, 'colsample_bylevel': 0.7, 'colsample_bynode': 0.8}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:34:21,167]\u001b[0m Trial 1 finished with value: 32.49035355912959 and parameters: {'learning_rate': 0.13, 'max_depth': 3, 'min_child_weight': 18, 'gamma': 17.7, 'alpha': 1.5, 'lambda': 52.078074880766074, 'colsample_bytree': 1.0, 'colsample_bylevel': 0.95, 'colsample_bynode': 0.5}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:34:54,794]\u001b[0m Trial 2 finished with value: 31.839921833008184 and parameters: {'learning_rate': 0.09, 'max_depth': 6, 'min_child_weight': 17, 'gamma': 12.5, 'alpha': 0.55, 'lambda': 0.001009788660292811, 'colsample_bytree': 1.0, 'colsample_bylevel': 0.55, 'colsample_bynode': 0.7}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:35:55,382]\u001b[0m Trial 3 finished with value: 32.0956623849548 and parameters: {'learning_rate': 0.11, 'max_depth': 14, 'min_child_weight': 10, 'gamma': 16.6, 'alpha': 3.6, 'lambda': 0.008991115020080608, 'colsample_bytree': 0.8, 'colsample_bylevel': 0.55, 'colsample_bynode': 0.5}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:37:52,869]\u001b[0m Trial 4 finished with value: 31.29325388786314 and parameters: {'learning_rate': 0.25, 'max_depth': 9, 'min_child_weight': 5, 'gamma': 11.0, 'alpha': 2.95, 'lambda': 1544.9177132470525, 'colsample_bytree': 0.8500000000000001, 'colsample_bylevel': 0.8, 'colsample_bynode': 0.55}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:37:53,051]\u001b[0m Trial 5 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:37:53,238]\u001b[0m Trial 6 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:53:47,460]\u001b[0m Trial 7 finished with value: 32.077048368636895 and parameters: {'learning_rate': 0.08, 'max_depth': 11, 'min_child_weight': 20, 'gamma': 5.1000000000000005, 'alpha': 2.25, 'lambda': 60275.69732684072, 'colsample_bytree': 0.6, 'colsample_bylevel': 0.6, 'colsample_bynode': 0.8500000000000001}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:53:48,172]\u001b[0m Trial 8 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:54:07,039]\u001b[0m Trial 9 finished with value: 32.185313861781395 and parameters: {'learning_rate': 0.12, 'max_depth': 4, 'min_child_weight': 4, 'gamma': 10.700000000000001, 'alpha': 2.5500000000000003, 'lambda': 10.672795533781748, 'colsample_bytree': 0.55, 'colsample_bylevel': 0.9, 'colsample_bynode': 0.8}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:54:07,250]\u001b[0m Trial 10 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:54:07,559]\u001b[0m Trial 11 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:54:07,755]\u001b[0m Trial 12 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:04,364]\u001b[0m Trial 13 finished with value: 32.01785742857224 and parameters: {'learning_rate': 0.27, 'max_depth': 10, 'min_child_weight': 7, 'gamma': 12.8, 'alpha': 2.1, 'lambda': 104.19814754450955, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.65, 'colsample_bynode': 0.7}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:07,087]\u001b[0m Trial 14 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:07,271]\u001b[0m Trial 15 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:07,471]\u001b[0m Trial 16 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:07,672]\u001b[0m Trial 17 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:07,891]\u001b[0m Trial 18 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:08,089]\u001b[0m Trial 19 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:08,293]\u001b[0m Trial 20 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:08,605]\u001b[0m Trial 21 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:08,802]\u001b[0m Trial 22 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:09,073]\u001b[0m Trial 23 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:09,257]\u001b[0m Trial 24 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:09,464]\u001b[0m Trial 25 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:09,662]\u001b[0m Trial 26 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:09,929]\u001b[0m Trial 27 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:10,159]\u001b[0m Trial 28 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:10,337]\u001b[0m Trial 29 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:10,530]\u001b[0m Trial 30 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:10,725]\u001b[0m Trial 31 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:12,636]\u001b[0m Trial 32 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:12,913]\u001b[0m Trial 33 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:14,843]\u001b[0m Trial 34 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:22,977]\u001b[0m Trial 35 pruned. Trial was pruned at iteration 243.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:23,450]\u001b[0m Trial 36 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:23,865]\u001b[0m Trial 37 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:24,275]\u001b[0m Trial 38 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:24,479]\u001b[0m Trial 39 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:25,097]\u001b[0m Trial 40 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:25,712]\u001b[0m Trial 41 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:26,001]\u001b[0m Trial 42 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:26,208]\u001b[0m Trial 43 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 05:55:26,557]\u001b[0m Trial 44 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:35,885]\u001b[0m Trial 45 finished with value: 31.4058089675813 and parameters: {'learning_rate': 0.27, 'max_depth': 8, 'min_child_weight': 5, 'gamma': 1.8, 'alpha': 2.8000000000000003, 'lambda': 65260.963354354775, 'colsample_bytree': 0.65, 'colsample_bylevel': 0.7, 'colsample_bynode': 0.95}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:36,083]\u001b[0m Trial 46 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:36,739]\u001b[0m Trial 47 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:36,963]\u001b[0m Trial 48 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:37,185]\u001b[0m Trial 49 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:37,434]\u001b[0m Trial 50 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:37,637]\u001b[0m Trial 51 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:37,862]\u001b[0m Trial 52 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:38,291]\u001b[0m Trial 53 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:38,531]\u001b[0m Trial 54 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:38,730]\u001b[0m Trial 55 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:39,046]\u001b[0m Trial 56 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:41,682]\u001b[0m Trial 57 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:42,813]\u001b[0m Trial 58 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:44,221]\u001b[0m Trial 59 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:45,481]\u001b[0m Trial 60 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:45,756]\u001b[0m Trial 61 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:47,011]\u001b[0m Trial 62 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:47,248]\u001b[0m Trial 63 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:06:47,592]\u001b[0m Trial 64 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:08:10,279]\u001b[0m Trial 65 finished with value: 31.474531465618394 and parameters: {'learning_rate': 0.03, 'max_depth': 6, 'min_child_weight': 16, 'gamma': 11.9, 'alpha': 2.7, 'lambda': 1.010712213682079, 'colsample_bytree': 0.6, 'colsample_bylevel': 0.8, 'colsample_bynode': 0.95}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:08:10,526]\u001b[0m Trial 66 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:08:11,072]\u001b[0m Trial 67 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:08:11,460]\u001b[0m Trial 68 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:08:11,690]\u001b[0m Trial 69 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:08:11,889]\u001b[0m Trial 70 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:08:12,159]\u001b[0m Trial 71 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:08:12,731]\u001b[0m Trial 72 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:08:12,936]\u001b[0m Trial 73 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:08:14,850]\u001b[0m Trial 74 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:08:16,145]\u001b[0m Trial 75 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:08:16,393]\u001b[0m Trial 76 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:18,071]\u001b[0m Trial 77 finished with value: 32.8045894537574 and parameters: {'learning_rate': 0.29000000000000004, 'max_depth': 12, 'min_child_weight': 13, 'gamma': 0.8, 'alpha': 4.9, 'lambda': 24.136431687451967, 'colsample_bytree': 0.75, 'colsample_bylevel': 0.9, 'colsample_bynode': 0.8500000000000001}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:18,367]\u001b[0m Trial 78 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:20,499]\u001b[0m Trial 79 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:20,735]\u001b[0m Trial 80 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:21,165]\u001b[0m Trial 81 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:21,405]\u001b[0m Trial 82 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:21,843]\u001b[0m Trial 83 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:22,062]\u001b[0m Trial 84 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:22,266]\u001b[0m Trial 85 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:22,484]\u001b[0m Trial 86 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:26,428]\u001b[0m Trial 87 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:30,243]\u001b[0m Trial 88 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:30,500]\u001b[0m Trial 89 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:31,994]\u001b[0m Trial 90 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:32,899]\u001b[0m Trial 91 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:33,701]\u001b[0m Trial 92 pruned. Trial was pruned at iteration 243.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:44,107]\u001b[0m Trial 93 finished with value: 32.86428642963749 and parameters: {'learning_rate': 0.19, 'max_depth': 4, 'min_child_weight': 2, 'gamma': 1.7000000000000002, 'alpha': 1.35, 'lambda': 0.546995893870647, 'colsample_bytree': 0.9, 'colsample_bylevel': 0.9, 'colsample_bynode': 0.55}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:47,130]\u001b[0m Trial 94 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:49,183]\u001b[0m Trial 95 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:49,967]\u001b[0m Trial 96 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:50,405]\u001b[0m Trial 97 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:50,632]\u001b[0m Trial 98 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:51,646]\u001b[0m Trial 99 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:51,857]\u001b[0m Trial 100 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:52,546]\u001b[0m Trial 101 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:52,771]\u001b[0m Trial 102 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:09:52,986]\u001b[0m Trial 103 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:30,940]\u001b[0m Trial 104 finished with value: 32.0213671723776 and parameters: {'learning_rate': 0.15000000000000002, 'max_depth': 11, 'min_child_weight': 18, 'gamma': 7.800000000000001, 'alpha': 1.55, 'lambda': 2.1630877364959007, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.8, 'colsample_bynode': 1.0}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:31,159]\u001b[0m Trial 105 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:32,013]\u001b[0m Trial 106 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:32,361]\u001b[0m Trial 107 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:33,205]\u001b[0m Trial 108 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:33,614]\u001b[0m Trial 109 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:33,858]\u001b[0m Trial 110 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:35,455]\u001b[0m Trial 111 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:35,675]\u001b[0m Trial 112 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:36,362]\u001b[0m Trial 113 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:36,646]\u001b[0m Trial 114 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:36,858]\u001b[0m Trial 115 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:37,173]\u001b[0m Trial 116 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:40,239]\u001b[0m Trial 117 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:41,149]\u001b[0m Trial 118 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:41,567]\u001b[0m Trial 119 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:42,673]\u001b[0m Trial 120 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:43,097]\u001b[0m Trial 121 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:43,630]\u001b[0m Trial 122 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:44,100]\u001b[0m Trial 123 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:44,634]\u001b[0m Trial 124 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:44,957]\u001b[0m Trial 125 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:45,171]\u001b[0m Trial 126 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:45,856]\u001b[0m Trial 127 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:47,059]\u001b[0m Trial 128 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:47,410]\u001b[0m Trial 129 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:47,892]\u001b[0m Trial 130 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:48,110]\u001b[0m Trial 131 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:50,373]\u001b[0m Trial 132 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:50,796]\u001b[0m Trial 133 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:51,384]\u001b[0m Trial 134 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:57,476]\u001b[0m Trial 135 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:57,795]\u001b[0m Trial 136 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:10:58,435]\u001b[0m Trial 137 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:01,119]\u001b[0m Trial 138 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:02,417]\u001b[0m Trial 139 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:03,354]\u001b[0m Trial 140 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:03,833]\u001b[0m Trial 141 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:04,286]\u001b[0m Trial 142 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:04,703]\u001b[0m Trial 143 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:04,915]\u001b[0m Trial 144 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:05,770]\u001b[0m Trial 145 pruned. Trial was pruned at iteration 243.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:08,131]\u001b[0m Trial 146 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:10,611]\u001b[0m Trial 147 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:10,985]\u001b[0m Trial 148 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:11,221]\u001b[0m Trial 149 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:11,472]\u001b[0m Trial 150 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:16,466]\u001b[0m Trial 151 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:16,900]\u001b[0m Trial 152 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:17,574]\u001b[0m Trial 153 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:20,177]\u001b[0m Trial 154 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:20,980]\u001b[0m Trial 155 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:21,420]\u001b[0m Trial 156 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:21,639]\u001b[0m Trial 157 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:26,655]\u001b[0m Trial 158 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:26,912]\u001b[0m Trial 159 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:27,450]\u001b[0m Trial 160 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:28,783]\u001b[0m Trial 161 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:30,070]\u001b[0m Trial 162 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:32,395]\u001b[0m Trial 163 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:32,834]\u001b[0m Trial 164 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:33,078]\u001b[0m Trial 165 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:33,288]\u001b[0m Trial 166 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:33,531]\u001b[0m Trial 167 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:33,755]\u001b[0m Trial 168 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:34,221]\u001b[0m Trial 169 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:34,473]\u001b[0m Trial 170 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:34,710]\u001b[0m Trial 171 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:38,157]\u001b[0m Trial 172 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:39,029]\u001b[0m Trial 173 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:39,234]\u001b[0m Trial 174 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:39,497]\u001b[0m Trial 175 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:39,845]\u001b[0m Trial 176 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:44,616]\u001b[0m Trial 177 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:11:44,843]\u001b[0m Trial 178 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:13,217]\u001b[0m Trial 179 finished with value: 33.4165434379976 and parameters: {'learning_rate': 0.29000000000000004, 'max_depth': 14, 'min_child_weight': 17, 'gamma': 15.100000000000001, 'alpha': 0.7000000000000001, 'lambda': 0.0013239702702222199, 'colsample_bytree': 0.8500000000000001, 'colsample_bylevel': 0.65, 'colsample_bynode': 0.8}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:17,089]\u001b[0m Trial 180 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:18,132]\u001b[0m Trial 181 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:19,209]\u001b[0m Trial 182 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:19,840]\u001b[0m Trial 183 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:20,157]\u001b[0m Trial 184 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:20,540]\u001b[0m Trial 185 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:20,775]\u001b[0m Trial 186 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:21,025]\u001b[0m Trial 187 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:21,271]\u001b[0m Trial 188 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:32,251]\u001b[0m Trial 189 pruned. Trial was pruned at iteration 243.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:33,130]\u001b[0m Trial 190 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:34,048]\u001b[0m Trial 191 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:34,307]\u001b[0m Trial 192 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:34,637]\u001b[0m Trial 193 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:36,146]\u001b[0m Trial 194 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:12:37,870]\u001b[0m Trial 195 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:05,891]\u001b[0m Trial 196 finished with value: 33.31326222267066 and parameters: {'learning_rate': 0.29000000000000004, 'max_depth': 14, 'min_child_weight': 18, 'gamma': 15.200000000000001, 'alpha': 0.65, 'lambda': 0.0022539284964256134, 'colsample_bytree': 0.8500000000000001, 'colsample_bylevel': 0.65, 'colsample_bynode': 0.8}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:06,109]\u001b[0m Trial 197 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:08,151]\u001b[0m Trial 198 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:08,606]\u001b[0m Trial 199 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:10,826]\u001b[0m Trial 200 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:11,113]\u001b[0m Trial 201 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:11,356]\u001b[0m Trial 202 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:12,599]\u001b[0m Trial 203 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:15,607]\u001b[0m Trial 204 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:16,045]\u001b[0m Trial 205 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:17,144]\u001b[0m Trial 206 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:18,342]\u001b[0m Trial 207 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:18,953]\u001b[0m Trial 208 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:19,335]\u001b[0m Trial 209 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:19,583]\u001b[0m Trial 210 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:19,933]\u001b[0m Trial 211 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:22,414]\u001b[0m Trial 212 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:24,140]\u001b[0m Trial 213 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:24,415]\u001b[0m Trial 214 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:25,450]\u001b[0m Trial 215 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:27,059]\u001b[0m Trial 216 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:30,798]\u001b[0m Trial 217 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:31,631]\u001b[0m Trial 218 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:32,408]\u001b[0m Trial 219 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:32,839]\u001b[0m Trial 220 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:34,881]\u001b[0m Trial 221 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:35,670]\u001b[0m Trial 222 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:36,149]\u001b[0m Trial 223 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:36,455]\u001b[0m Trial 224 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:36,674]\u001b[0m Trial 225 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:37,503]\u001b[0m Trial 226 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:37,725]\u001b[0m Trial 227 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:38,118]\u001b[0m Trial 228 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:41,713]\u001b[0m Trial 229 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:42,817]\u001b[0m Trial 230 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:43,290]\u001b[0m Trial 231 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:43,747]\u001b[0m Trial 232 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:44,220]\u001b[0m Trial 233 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:48,328]\u001b[0m Trial 234 pruned. Trial was pruned at iteration 81.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:48,557]\u001b[0m Trial 235 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:48,789]\u001b[0m Trial 236 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:13:49,271]\u001b[0m Trial 237 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:14:51,237]\u001b[0m Trial 238 finished with value: 32.215838608263184 and parameters: {'learning_rate': 0.24000000000000002, 'max_depth': 13, 'min_child_weight': 17, 'gamma': 2.9000000000000004, 'alpha': 5.0, 'lambda': 32.811098384322214, 'colsample_bytree': 0.7, 'colsample_bylevel': 0.8500000000000001, 'colsample_bynode': 0.8}. Best is trial 0 with value: 31.20323621889596.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:14:51,845]\u001b[0m Trial 239 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:14:52,573]\u001b[0m Trial 240 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:14:53,496]\u001b[0m Trial 241 pruned. Trial was pruned at iteration 27.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:14:53,717]\u001b[0m Trial 242 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:14:55,013]\u001b[0m Trial 243 pruned. Trial was pruned at iteration 9.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:14:55,378]\u001b[0m Trial 244 pruned. Trial was pruned at iteration 1.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:14:55,796]\u001b[0m Trial 245 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:14:56,258]\u001b[0m Trial 246 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:14:56,478]\u001b[0m Trial 247 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:14:56,891]\u001b[0m Trial 248 pruned. Trial was pruned at iteration 3.\u001b[0m\n",
            "\u001b[32m[I 2022-09-24 06:14:57,220]\u001b[0m Trial 249 pruned. Trial was pruned at iteration 1.\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best trial: 0 -> Best value(RMSE): 31.20324\n",
            "Best hyperparameters:\n",
            "learning_rate        - 0.16\n",
            "max_depth            - 15\n",
            "min_child_weight     - 16\n",
            "gamma                - 5.6000000000000005\n",
            "alpha                - 1.1\n",
            "lambda               - 308.87067834937415\n",
            "colsample_bytree     - 0.55\n",
            "colsample_bylevel    - 0.7\n",
            "colsample_bynode     - 0.8\n",
            "-----------------Cross-validation------------------\n",
            "Fold #0: (1026 rounds) RMSE = 33.31880\n",
            "Fold #1: (821 rounds) RMSE = 30.42948\n",
            "Fold #2: (621 rounds) RMSE = 27.55059\n",
            "Fold #3: (663 rounds) RMSE = 36.33202\n",
            "Fold #4: (521 rounds) RMSE = 28.38529\n",
            "\n",
            "Avg RMSE = 31.20324 +/- 3.24535\n",
            "OOF RMSE = 31.36913\n",
            "CPU times: user 48min 5s, sys: 8.48 s, total: 48min 14s\n",
            "Wall time: 47min 38s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for col in (test_preds.columns):\n",
        "    sub = pd.DataFrame({'Place_ID X Date': TEST_INDEX, 'target': test_preds[col]})\n",
        "    sub.to_csv(f'{SUBMISSION_PATH}/{col}.csv', index=False)\n",
        "\n",
        "sub = pd.DataFrame({'Place_ID X Date': TEST_INDEX, 'target': test_preds.mean(axis=1)})\n",
        "sub.to_csv(f'{SUBMISSION_PATH}/mean.csv', index=False)"
      ],
      "metadata": {
        "id": "6JdT3E2Bp4-k"
      },
      "execution_count": 38,
      "outputs": []
    }
  ]
}